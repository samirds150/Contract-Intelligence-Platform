<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Contract RAG - Full Project Guide</title>
    <style>
        * {
            margin: 0;
            padding: 0;
            box-sizing: border-box;
        }
        
        html {
            scroll-behavior: smooth;
        }
        
        body {
            font-family: -apple-system, BlinkMacSystemFont, 'Segoe UI', Roboto, Oxygen, Ubuntu, Cantarell, sans-serif;
            line-height: 1.7;
            color: #333;
            background: #fafbfc;
            padding: 40px 20px;
        }
        
        .container {
            max-width: 900px;
            margin: 0 auto;
            background: white;
            padding: 50px;
            border-radius: 8px;
            box-shadow: 0 2px 8px rgba(0, 0, 0, 0.1);
        }
        
        @media print {
            body {
                padding: 0;
                background: white;
            }
            .container {
                max-width: 100%;
                padding: 30px;
                box-shadow: none;
                border-radius: 0;
            }
        }
        
        h1 {
            font-size: 2.8em;
            font-weight: 700;
            margin: 0.3em 0 0.4em 0;
            color: #1a1a1a;
            border-bottom: 3px solid #0066cc;
            padding-bottom: 0.3em;
            letter-spacing: -0.5px;
        }
        
        h2 {
            font-size: 2em;
            font-weight: 600;
            margin: 1.3em 0 0.5em 0;
            color: #0066cc;
            padding-top: 0.5em;
        }
        
        h3 {
            font-size: 1.45em;
            font-weight: 600;
            margin: 1em 0 0.4em 0;
            color: #0088dd;
        }
        
        p {
            margin: 0.9em 0;
            text-align: left;
            word-wrap: break-word;
        }
        
        code {
            background-color: #f0f0f0;
            color: #d73a49;
            padding: 3px 8px;
            border-radius: 3px;
            font-family: 'SF Mono', 'Monaco', 'Courier New', monospace;
            font-size: 0.9em;
            border: 1px solid #e1e4e8;
            display: inline-block;
            white-space: nowrap;
        }
        
        pre {
            background-color: #2d2d2d;
            color: #f8f8f2;
            padding: 18px;
            border-radius: 6px;
            overflow-x: auto;
            margin: 1.2em 0;
            font-family: 'SF Mono', 'Monaco', 'Courier New', monospace;
            font-size: 0.85em;
            line-height: 1.5;
            border-left: 5px solid #0066cc;
            box-shadow: 0 2px 6px rgba(0, 0, 0, 0.15);
        }
        
        pre code {
            background-color: transparent;
            color: inherit;
            padding: 0;
            border-radius: 0;
            border: none;
            font-size: inherit;
            display: block;
            white-space: pre;
        }
        
        ul {
            margin: 1em 0;
            padding-left: 0;
        }
        
        li {
            margin: 0.6em 0 0.6em 2.5em;
            line-height: 1.7;
            text-align: left;
        }
        
        a {
            color: #0066cc;
            text-decoration: none;
            border-bottom: 1px dotted #0066cc;
            transition: color 0.2s;
        }
        
        a:hover {
            color: #0052a3;
            text-decoration: underline;
        }
        
        hr {
            border: none;
            height: 2px;
            background: linear-gradient(to right, transparent, #0066cc, transparent);
            margin: 2.5em 0;
        }
        
        strong {
            font-weight: 700;
            color: #1a1a1a;
        }
        
        em {
            font-style: italic;
            color: #555;
        }
        
        @media (max-width: 768px) {
            .container {
                padding: 25px;
            }
            
            h1 {
                font-size: 2em;
                margin: 0.2em 0 0.3em 0;
            }
            
            h2 {
                font-size: 1.5em;
                margin: 1em 0 0.4em 0;
            }
            
            h3 {
                font-size: 1.2em;
                margin: 0.8em 0 0.3em 0;
            }
            
            pre {
                font-size: 0.75em;
                padding: 12px;
                margin: 1em 0;
            }
            
            li {
                margin-left: 1.5em;
            }
        }
    </style>
</head>
<body>
    <div class="container">
        <h1>Complete Functions Reference Guide</h1>

<p>This document provides detailed explanations of every function in the Contract RAG system.</p>

<hr>

<h2>Table of Contents</h2>
<p>1. <a href="#data_processor">src/data_processor.py</a></p>
<p>2. <a href="#embeddings">src/embeddings.py</a></p>
<p>3. <a href="#rag_system">src/rag_system.py</a></p>
<p>4. <a href="#views">ragapp/views.py</a></p>
<p>5. <a href="#query">query.py</a></p>
<p>6. <a href="#setup_knowledge_base">setup_knowledge_base.py</a></p>

<hr>

<h2>&lt;a name=&quot;data_processor&quot;&gt;&lt;/a&gt;1. src/data_processor.py</h2>

<p><strong>Purpose</strong>: Handles document loading, text cleaning, and chunking for the RAG system.</p>

<h3>Class: DataProcessor</h3>

<p>#### Function 1: <code>__init__(chunk_size: int = 512, chunk_overlap: int = 100)</code></p>

<p><strong>What it does</strong>: Initializes the DataProcessor with chunk configuration parameters.</p>

<p><strong>Parameters</strong>:</p>
<ul>
<li><code>chunk_size</code> (int, default=512): Number of characters per chunk</li>
<li><code>chunk_overlap</code> (int, default=100): Number of overlapping characters between consecutive chunks</li>
</ul>

<p><strong>Why overlap matters</strong>:</p>
<ul>
<li>Without overlap: If a sentence is split between chunks, information is lost</li>
<li>With 100-char overlap: Important context is preserved across chunk boundaries</li>
<li>Example: If a sentence spans chars 500-520, with overlap=100, it will appear in both chunk ending at 612 and chunk starting at 512</li>
</ul>

<p><strong>Returns</strong>: None (initializes instance variables)</p>

<p><strong>Example usage</strong>:</p>
<pre><code class="language-python">processor = DataProcessor(chunk_size=400, chunk_overlap=50)</code></pre>

<hr>

<p>#### Function 2: <code>load_documents(data_path: str) -&gt; List[Dict[str, str]]</code></p>

<p><strong>What it does</strong>: Reads all <code>.txt</code> files from a directory and returns them as a list of dictionaries.</p>

<p><strong>Parameters</strong>:</p>
<ul>
<li><code>data_path</code> (str): Path to directory containing <code>.txt</code> contract files</li>
</ul>

<p><strong>Process flow</strong>:</p>
<p>1. Checks if directory exists</p>
<p>2. Finds all <code>.txt</code> files in the directory</p>
<p>3. Reads each file with UTF-8 encoding</p>
<p>4. Creates a dictionary with <code>filename</code> and <code>content</code> keys</p>
<p>5. Returns list of dictionaries</p>

<p><strong>Error handling</strong>:</p>
<ul>
<li>Raises <code>FileNotFoundError</code> if directory doesn't exist</li>
<li>Raises <code>FileNotFoundError</code> if no <code>.txt</code> files found</li>
<li>Catches and prints errors for individual files (doesn't stop entire process)</li>
</ul>

<p><strong>Returns</strong>: List of dictionaries</p>
<pre><code class="language-python">[
    {'filename': 'contract1.txt', 'content': '...full text...'},
    {'filename': 'contract2.txt', 'content': '...full text...'},
]</code></pre>

<p><strong>Example</strong>:</p>
<pre><code class="language-python">processor = DataProcessor()
docs = processor.load_documents('data/raw')
# Returns documents with keys: 'filename', 'content'</code></pre>

<hr>

<p>#### Function 3: <code>clean_text(text: str) -&gt; str</code></p>

<p><strong>What it does</strong>: Normalizes and cleans raw text from contract files.</p>

<p><strong>Parameters</strong>:</p>
<ul>
<li><code>text</code> (str): Raw text to clean</li>
</ul>

<p><strong>Cleaning steps</strong>:</p>
<p>1. <strong>Remove extra whitespace</strong>: Replaces multiple spaces/tabs/newlines with single space</p>
<ul>
<li><code>&quot;Hello    world&quot;</code> → <code>&quot;Hello world&quot;</code></li>
<li><code>&quot;Line1\n\nLine2&quot;</code> → <code>&quot;Line1 Line2&quot;</code></li>
</ul>

<p>2. <strong>Remove control characters</strong>: Strips non-printable Unicode characters (0x00-0x1F except tab)</p>
<ul>
<li>Removes hidden formatting characters that PDFs might introduce</li>
</ul>

<p><strong>Why this matters</strong>:</p>
<ul>
<li>RAG systems need consistent text format for accurate embeddings</li>
<li>Multiple whitespaces can confuse the embedding model</li>
<li>Control characters can cause encoding issues</li>
</ul>

<p><strong>Returns</strong>: Cleaned string</p>

<p><strong>Example</strong>:</p>
<pre><code class="language-python">raw = &quot;Hello   world\n\nThis   is\t\ta test&quot;
cleaned = processor.clean_text(raw)
# Returns: &quot;Hello world This is a test&quot;</code></pre>

<hr>

<p>#### Function 4: <code>chunk_documents(documents: List[Dict[str, str]]) -&gt; List[Dict[str, str]]</code></p>

<p><strong>What it does</strong>: Splits documents into overlapping chunks with metadata preserved.</p>

<p><strong>Parameters</strong>:</p>
<ul>
<li><code>documents</code> (List[Dict]): List of documents from <code>load_documents()</code></li>
</ul>

<p><strong>Chunking logic</strong>:</p>
<p>1. For each document, cleans the text</p>
<p>2. Creates overlapping chunks using sliding window:</p>
<ul>
<li>First chunk: chars 0-512</li>
<li>Second chunk: chars (512-100) to (1024-100) = 412-1324</li>
<li>Maintains overlap of 100 characters</li>
</ul>
<p>3. Preserves metadata for each chunk:</p>
<ul>
<li><code>text</code>: The chunk content</li>
<li><code>source</code>: Which file it came from</li>
<li><code>chunk_id</code>: Sequential ID</li>
</ul>

<p><strong>Why overlapping chunks</strong>:</p>
<ul>
<li>Ensures queries don't miss information at chunk boundaries</li>
<li>Example: If query mentions &quot;Section 5&quot; at position 500, it will be in 2 overlapping chunks</li>
</ul>

<p><strong>Returns</strong>: List of chunk dictionaries</p>
<pre><code class="language-python">[
    {
        'text': '...512 char chunk...',
        'source': 'contract1.txt',
        'chunk_id': 0
    },
    {
        'text': '...512 char chunk...',
        'source': 'contract1.txt',
        'chunk_id': 1
    },
]</code></pre>

<p><strong>Example</strong>:</p>
<pre><code class="language-python">documents = processor.load_documents('data/raw')
chunks = processor.chunk_documents(documents)
# If 5 documents × 50KB each = ~250KB total
# With 512 char chunks + 100 overlap, creates ~500 chunks</code></pre>

<hr>

<p>#### Function 5: <code>process_pipeline(data_path: str) -&gt; List[Dict[str, str]]</code></p>

<p><strong>What it does</strong>: Combines all processing steps into one complete pipeline.</p>

<p><strong>Parameters</strong>:</p>
<ul>
<li><code>data_path</code> (str): Path to raw data directory</li>
</ul>

<p><strong>Process flow</strong>:</p>
<pre><code class="language-text">data_path → load_documents() → chunk_documents() → chunks list</code></pre>

<p><strong>Why a pipeline function</strong>:</p>
<ul>
<li>Single entry point for all processing</li>
<li>Easier to use: one call instead of three</li>
<li>Makes error handling simpler</li>
</ul>

<p><strong>Returns</strong>: List of processed chunks</p>

<p><strong>Example</strong>:</p>
<pre><code class="language-python">processor = DataProcessor(chunk_size=400, chunk_overlap=50)
chunks = processor.process_pipeline('data/raw')
# Automatically: loads → cleans → chunks</code></pre>

<hr>

<h2>&lt;a name=&quot;embeddings&quot;&gt;&lt;/a&gt;2. src/embeddings.py</h2>

<p><strong>Purpose</strong>: Manages vector embeddings using BERT-based SentenceTransformer and creates FAISS search index.</p>

<h3>Class: EmbeddingManager</h3>

<p>#### Function 1: <code>__init__(model_name: str, device: str = &quot;cpu&quot;)</code></p>

<p><strong>What it does</strong>: Initializes the embedding model and prepares for encoding documents.</p>

<p><strong>Parameters</strong>:</p>
<ul>
<li><code>model_name</code> (str): HuggingFace model identifier, e.g., <code>&quot;sentence-transformers/all-MiniLM-L6-v2&quot;</code></li>
<li><code>device</code> (str, default=&quot;cpu&quot;): Either <code>&quot;cpu&quot;</code> or <code>&quot;cuda&quot;</code> (for GPU)</li>
</ul>

<p><strong>Process</strong>:</p>
<p>1. Loads pre-trained BERT model from HuggingFace</p>
<p>2. Moves model to specified device (CPU or GPU)</p>
<p>3. Stores embedding dimension (usually 384 for MiniLM)</p>

<p><strong>Why MiniLM</strong>:</p>
<ul>
<li>Fast: Optimized BERT distillation</li>
<li>CPU-friendly: Small model, runs without GPU</li>
<li>Accurate: Good performance on semantic similarity</li>
<li>Dimension: 384-dimensional vectors (vs 768 for full BERT)</li>
</ul>

<p><strong>Returns</strong>: None (initializes instance)</p>

<p><strong>Behind the scenes</strong>:</p>
<ul>
<li>First run downloads model (~90MB) to local cache</li>
<li>Subsequent runs load from cache (fast)</li>
</ul>

<p><strong>Example</strong>:</p>
<pre><code class="language-python">embedder = EmbeddingManager(
    model_name=&quot;sentence-transformers/all-MiniLM-L6-v2&quot;,
    device=&quot;cpu&quot;
)
# Model loaded and ready to encode documents</code></pre>

<hr>

<p>#### Function 2: <code>encode_documents(chunks: List[Dict[str, str]], batch_size: int = 32) -&gt; np.ndarray</code></p>

<p><strong>What it does</strong>: Converts text chunks into numerical vector embeddings using the transformer model.</p>

<p><strong>Parameters</strong>:</p>
<ul>
<li><code>chunks</code> (List[Dict]): List of chunk dictionaries from DataProcessor (with 'text' key)</li>
<li><code>batch_size</code> (int, default=32): How many chunks to process at once</li>
<li>Larger batch = faster but more memory</li>
<li>Smaller batch = slower but uses less memory</li>
</ul>

<p><strong>Process</strong>:</p>
<p>1. Extracts text from each chunk dictionary</p>
<p>2. Passes all texts to the BERT model</p>
<p>3. Model generates 384-dimensional vectors for each text</p>
<p>4. Returns as NumPy array (float32)</p>

<p><strong>Example vectors</strong>:</p>
<ul>
<li>&quot;payment terms&quot; → <code>[-0.234, 0.567, -0.123, ..., 0.456]</code> (384 values)</li>
<li>&quot;liability clause&quot; → <code>[-0.211, 0.598, -0.145, ..., 0.434]</code> (384 values)</li>
</ul>

<p><strong>Why float32</strong>:</p>
<ul>
<li>Uses less memory than float64 (8 bytes vs 16 bytes per value)</li>
<li>FAISS expects float32 format</li>
</ul>

<p><strong>Returns</strong>: NumPy array of shape (num_chunks, 384)</p>

<p><strong>Example</strong>:</p>
<pre><code class="language-python">chunks = processor.chunk_documents(documents)
embeddings = embedder.encode_documents(chunks, batch_size=32)
# embeddings.shape = (500, 384)  # 500 chunks, 384-dim vectors</code></pre>

<hr>

<p>#### Function 3: <code>build_index(embeddings: np.ndarray, metadata: List[Dict[str, str]]) -&gt; None</code></p>

<p><strong>What it does</strong>: Creates a FAISS index for fast similarity search over embeddings.</p>

<p><strong>Parameters</strong>:</p>
<ul>
<li><code>embeddings</code> (np.ndarray): Output from <code>encode_documents()</code></li>
<li><code>metadata</code> (List[Dict]): Original chunk dictionaries (with source, chunk_id, etc.)</li>
</ul>

<p><strong>FAISS Index Explanation</strong>:</p>
<ul>
<li>FAISS = Facebook AI Similarity Search</li>
<li><code>IndexFlatL2</code>: Uses Euclidean distance (L2) to find similar vectors</li>
<li>Fast: Can search 1M vectors in milliseconds</li>
<li>Stores embeddings in optimized format</li>
</ul>

<p><strong>Process</strong>:</p>
<p>1. Creates empty FAISS index with 384 dimensions</p>
<p>2. Adds all embeddings to the index</p>
<p>3. Stores metadata separately (FAISS only stores vectors, not text)</p>

<p><strong>Why separate metadata</strong>:</p>
<ul>
<li>FAISS: Stores only numerical vectors (fast, small)</li>
<li>Metadata: Stores text, source, chunk_id (needed for results)</li>
</ul>

<p><strong>Returns</strong>: None (modifies instance)</p>

<p><strong>Example</strong>:</p>
<pre><code class="language-python">embedder.build_index(embeddings, chunks)
# Now ready to search: embedder.search(&quot;payment terms&quot;)</code></pre>

<hr>

<p>#### Function 4: <code>search(query: str, top_k: int = 5) -&gt; List[Dict]</code></p>

<p><strong>What it does</strong>: Finds the top-k most similar chunks to a given query.</p>

<p><strong>Parameters</strong>:</p>
<ul>
<li><code>query</code> (str): User question or search text, e.g., &quot;What are the payment terms?&quot;</li>
<li><code>top_k</code> (int, default=5): Number of similar chunks to retrieve</li>
</ul>

<p><strong>Search process</strong>:</p>
<p>1. Encodes the query using same model (same 384-dim embedding)</p>
<p>2. Compares query embedding against all chunk embeddings using L2 distance</p>
<p>3. Returns indices of closest chunks</p>
<p>4. Calculates similarity score: <code>1 / (1 + distance)</code></p>
<ul>
<li>Small distance → high similarity (score close to 1)</li>
<li>Large distance → low similarity (score close to 0)</li>
</ul>

<p><strong>Similarity scoring</strong>:</p>
<pre><code class="language-text">L2 distance = sqrt((v1-v2)^2)
Similarity = 1 / (1 + distance)

Example:
- Distance 0.1 → Similarity = 1/(1+0.1) = 0.909
- Distance 1.0 → Similarity = 1/(1+1.0) = 0.500
- Distance 5.0 → Similarity = 1/(1+5.0) = 0.167</code></pre>

<p><strong>Returns</strong>: List of dictionaries with results</p>
<pre><code class="language-python">[
    {
        'text': 'Payment must be made within 30 days...',
        'source': 'contract1.txt',
        'chunk_id': 42,
        'similarity': 0.856
    },
    ...
]</code></pre>

<p><strong>Example</strong>:</p>
<pre><code class="language-python">results = embedder.search(&quot;payment deadline&quot;, top_k=3)
# Returns 3 most similar chunks</code></pre>

<hr>

<p>#### Function 5: <code>save_index(index_path: str, metadata_path: str) -&gt; None</code></p>

<p><strong>What it does</strong>: Saves the FAISS index and metadata to disk for later use.</p>

<p><strong>Parameters</strong>:</p>
<ul>
<li><code>index_path</code> (str): Path to save FAISS binary file (e.g., <code>&quot;models/faiss_index.bin&quot;</code>)</li>
<li><code>metadata_path</code> (str): Path to save metadata pickle (e.g., <code>&quot;models/metadata.pkl&quot;</code>)</li>
</ul>

<p><strong>What gets saved</strong>:</p>
<p>1. <strong>FAISS index</strong> (binary file): All embeddings in optimized format</p>
<ul>
<li>Size: ~1-10MB typically (depends on number of chunks)</li>
</ul>
<p>2. <strong>Metadata</strong> (pickle file): Text, sources, chunk IDs</p>
<ul>
<li>Size: Similar to compressed text</li>
</ul>

<p><strong>Why save to disk</strong>:</p>
<ul>
<li>Don't recreate embeddings every run (takes 30-60 seconds)</li>
<li>Keep knowledge base persistent</li>
<li>Load in seconds instead</li>
</ul>

<p><strong>Returns</strong>: None</p>

<p><strong>Example</strong>:</p>
<pre><code class="language-python">embedder.save_index('models/faiss_index.bin', 'models/metadata.pkl')
# Files now on disk, ready to load later</code></pre>

<hr>

<p>#### Function 6: <code>load_index(index_path: str, metadata_path: str) -&gt; None</code></p>

<p><strong>What it does</strong>: Loads previously saved FAISS index and metadata from disk.</p>

<p><strong>Parameters</strong>:</p>
<ul>
<li><code>index_path</code> (str): Path to saved FAISS index file</li>
<li><code>metadata_path</code> (str): Path to saved metadata pickle file</li>
</ul>

<p><strong>Process</strong>:</p>
<p>1. Checks if both files exist</p>
<p>2. Loads FAISS binary index</p>
<p>3. Loads metadata from pickle</p>
<p>4. Index is now ready for searching</p>

<p><strong>Speed comparison</strong>:</p>
<ul>
<li>Building index: 30-60 seconds (encoding, building FAISS)</li>
<li>Loading index: 2-5 seconds (just reading from disk)</li>
</ul>

<p><strong>Error handling</strong>: Raises <code>FileNotFoundError</code> if files don't exist</p>

<p><strong>Returns</strong>: None</p>

<p><strong>Example</strong>:</p>
<pre><code class="language-python">embedder.load_index('models/faiss_index.bin', 'models/metadata.pkl')
# Now can immediately search without rebuilding</code></pre>

<hr>

<h2>&lt;a name=&quot;rag_system&quot;&gt;&lt;/a&gt;3. src/rag_system.py</h2>

<p><strong>Purpose</strong>: Orchestrates the complete RAG pipeline combining retrieval and question-answering.</p>

<h3>Class: ContractRAG</h3>

<p>#### Function 1: <code>__init__(config: Dict)</code></p>

<p><strong>What it does</strong>: Initializes all RAG components with configuration.</p>

<p><strong>Parameters</strong>:</p>
<ul>
<li><code>config</code> (Dict): Configuration dictionary with structure:</li>
<pre><code class="language-python">{
    'data': {'chunk_size': 400, 'chunk_overlap': 50},
    'embedding': {
        'model_name': 'sentence-transformers/all-MiniLM-L6-v2',
        'faiss_index_path': 'models/faiss_index.bin',
        'metadata_path': 'models/metadata.pkl'
    },
    'rag': {'top_k': 3, 'similarity_threshold': 0.0},
    'model': {'device': 'cpu'}
}</code></pre>
</ul>

<p><strong>Initialization steps</strong>:</p>
<p>1. Extracts device (CPU/GPU) from config</p>
<p>2. Creates DataProcessor with chunk size/overlap</p>
<p>3. Creates EmbeddingManager with model and device</p>
<p>4. Loads QA transformer pipeline</p>

<p><strong>QA Model</strong> (<code>deepset/minilm-uncased-squad2</code>):</p>
<ul>
<li>Extractive QA: Finds answer within provided context</li>
<li>Reads question and context, highlights the answer span</li>
<li>Provides confidence score</li>
<li>Runs on CPU</li>
</ul>

<p><strong>Returns</strong>: None (initializes RAG system)</p>

<p><strong>Example</strong>:</p>
<pre><code class="language-python">rag = ContractRAG(config)
# All components loaded and ready</code></pre>

<hr>

<p>#### Function 2: <code>build_knowledge_base(data_path: str) -&gt; None</code></p>

<p><strong>What it does</strong>: Builds complete knowledge base from raw contract files.</p>

<p><strong>Parameters</strong>:</p>
<ul>
<li><code>data_path</code> (str): Path to directory with <code>.txt</code> contracts</li>
</ul>

<p><strong>Process flow</strong>:</p>
<pre><code class="language-text">Raw contracts → Process pipeline → Chunks 
    → Embeddings → FAISS index → Save to disk</code></pre>

<p><strong>Step by step</strong>:</p>
<p>1. <code>data_processor.process_pipeline()</code>: Load, clean, chunk documents</p>
<p>2. <code>embedding_manager.encode_documents()</code>: Convert chunks to vectors</p>
<p>3. <code>embedding_manager.build_index()</code>: Create FAISS index</p>
<p>4. <code>embedding_manager.save_index()</code>: Save to disk</p>

<p><strong>Returns</strong>: None (saves to disk, ready for search)</p>

<p><strong>Example</strong>:</p>
<pre><code class="language-python">rag.build_knowledge_base('data/raw')
# Creates: models/faiss_index.bin, models/metadata.pkl
# (~30-60 seconds for 5 small contracts)</code></pre>

<hr>

<p>#### Function 3: <code>load_knowledge_base() -&gt; None</code></p>

<p><strong>What it does</strong>: Loads previously built knowledge base from disk.</p>

<p><strong>Parameters</strong>: None</p>

<p><strong>Process</strong>:</p>
<ul>
<li>Calls <code>embedding_manager.load_index()</code> with paths from config</li>
</ul>

<p><strong>Speed</strong>: 2-5 seconds</p>

<p><strong>Returns</strong>: None</p>

<p><strong>Example</strong>:</p>
<pre><code class="language-python">rag.load_knowledge_base()
# Index loaded, ready to search immediately</code></pre>

<hr>

<p>#### Function 4: <code>retrieve_context(query: str, top_k: int = None) -&gt; List[Dict]</code></p>

<p><strong>What it does</strong>: Retrieves top-k relevant chunks for a query using semantic search.</p>

<p><strong>Parameters</strong>:</p>
<ul>
<li><code>query</code> (str): User question</li>
<li><code>top_k</code> (int, optional): Number of chunks to retrieve</li>
<li>If None, uses value from config (default 3)</li>
</ul>

<p><strong>Process</strong>:</p>
<p>1. Searches FAISS index for top-k similar chunks</p>
<p>2. Filters by similarity threshold from config</p>
<ul>
<li>Only returns chunks with similarity ≥ threshold</li>
<li>Default threshold = 0.0 (keeps all results)</li>
</ul>
<p>3. Returns filtered list</p>

<p><strong>Threshold filtering</strong>:</p>
<ul>
<li>Threshold 0.0: Keeps all results (more context but potentially noisy)</li>
<li>Threshold 0.5: Keeps only chunks with 50%+ similarity (stricter)</li>
<li>Threshold 0.8: Very strict, only highly relevant chunks</li>
</ul>

<p><strong>Returns</strong>: List of chunk dictionaries with similarity scores</p>

<p><strong>Example</strong>:</p>
<pre><code class="language-python">chunks = rag.retrieve_context(&quot;payment deadline&quot;, top_k=3)
# Returns list of 3 most similar chunks (or fewer if filtered)</code></pre>

<hr>

<p>#### Function 5: <code>answer_question(question: str, top_k: int = 3) -&gt; Dict</code></p>

<p><strong>What it does</strong>: Complete Q&amp;A pipeline: retrieve context and generate answer.</p>

<p><strong>Parameters</strong>:</p>
<ul>
<li><code>question</code> (str): User question</li>
<li><code>top_k</code> (int, default=3): Number of context chunks to use</li>
</ul>

<p><strong>Process</strong>:</p>
<p>1. <code>retrieve_context()</code>: Get top-k relevant chunks</p>
<p>2. <strong>No chunks found</strong>: Return &quot;No relevant information found&quot;</p>
<p>3. <strong>Chunks found</strong>:</p>
<ul>
<li>Concatenate all chunk texts as context</li>
<li>Pass to QA model: <code>qa_pipeline(question=question, context=context)</code></li>
<li>QA model extraction answer span and confidence score</li>
<li>Compile response with answer, confidence, and sources</li>
</ul>

<p><strong>Error handling</strong>: Catches exceptions and returns error message</p>

<p><strong>Returns</strong>: Dictionary with structure:</p>
<pre><code class="language-python">{
    'answer': 'The payment is due within 30 days of invoice...',
    'confidence': 0.876,
    'context': [
        {
            'text': 'Truncated first 200 chars of chunk...',
            'source': 'contract1.txt',
            'similarity': 0.856
        }
    ],
    'sources': ['contract1.txt', 'contract2.txt']
}</code></pre>

<p><strong>Confidence score</strong>:</p>
<ul>
<li>0.0-1.0 range</li>
<li>Higher = higher confidence in answer</li>
<li>Based on QA model's internal scorer</li>
</ul>

<p><strong>Example</strong>:</p>
<pre><code class="language-python">result = rag.answer_question(&quot;When is payment due?&quot;)
print(f&quot;Answer: {result['answer']}&quot;)
print(f&quot;Confidence: {result['confidence']:.1%}&quot;)</code></pre>

<hr>

<h2>&lt;a name=&quot;views&quot;&gt;&lt;/a&gt;4. ragapp/views.py</h2>

<p><strong>Purpose</strong>: Django view functions handling HTTP requests for web UI.</p>

<h3>Utility Function: <code>get_rag_config() -&gt; Dict</code></h3>

<p><strong>What it does</strong>: Loads YAML config and wraps in nested structure expected by ContractRAG.</p>

<p><strong>Why needed</strong>:</p>
<ul>
<li>YAML config is flat: <code>chunk_size</code>, <code>models_path</code></li>
<li>ContractRAG expects nested: <code>config['data']['chunk_size']</code></li>
<li>This function bridges the gap</li>
</ul>

<p><strong>Process</strong>:</p>
<p>1. Reads <code>config/config.yaml</code></p>
<p>2. Restructures into nested dictionary</p>
<p>3. Returns properly formatted config</p>

<p><strong>Returns</strong>: Nested config dictionary</p>

<p><strong>Example</strong>:</p>
<pre><code class="language-python">config = get_rag_config()
# Returns properly nested structure for ContractRAG</code></pre>

<hr>

<h3>View Function 1: <code>index(request) -&gt; HttpResponse</code></h3>

<p><strong>What it does</strong>: Displays the main page with upload and query forms.</p>

<p><strong>Parameters</strong>:</p>
<ul>
<li><code>request</code>: Django HttpRequest object</li>
</ul>

<p><strong>Process</strong>:</p>
<p>1. Creates empty <code>UploadForm</code> (for file uploads)</p>
<p>2. Creates empty <code>AskForm</code> (for queries)</p>
<p>3. Gets list of uploaded files from media folder</p>
<p>4. Renders template with forms and file list</p>

<p><strong>Response</strong>: HTML page with:</p>
<ul>
<li>File upload form</li>
<li>Question input form</li>
<li>List of current uploaded files</li>
</ul>

<p><strong>Returns</strong>: Rendered HTML template</p>

<p><strong>HTTP method</strong>: GET</p>

<p><strong>Example URL</strong>: <code>http://localhost:8000/</code></p>

<hr>

<h3>View Function 2: <code>upload(request) -&gt; HttpResponse</code></h3>

<p><strong>What it does</strong>: Handles file uploads and rebuilds knowledge base.</p>

<p><strong>Parameters</strong>:</p>
<ul>
<li><code>request</code>: Django HttpRequest with file data</li>
</ul>

<p><strong>Process</strong>:</p>
<p>1. Check if POST request</p>
<p>2. Validate form with <code>UploadForm</code></p>
<p>3. If valid:</p>
<ul>
<li>Save uploaded file to media folder</li>
<li>Initialize RAG system</li>
<li>Rebuild knowledge base from all files in folder</li>
<li>Return success message</li>
</ul>
<p>4. If invalid or error: Return error message</p>

<p><strong>Process flow</strong>:</p>
<pre><code class="language-text">User uploads file → Saved to disk 
    → RAG.build_knowledge_base() → FAISS index rebuilt 
    → Success response</code></pre>

<p><strong>Key points</strong>:</p>
<ul>
<li>Rebuilds entire KB (not incremental)</li>
<li>All files in media folder included</li>
<li>Takes 30-60 seconds for large contracts</li>
</ul>

<p><strong>Returns</strong>: HTML template with result/error message</p>

<p><strong>HTTP method</strong>: POST</p>

<p><strong>Example</strong>: User uploads <code>new_contract.txt</code> → KB rebuilt</p>

<hr>

<h3>View Function 3: <code>ask(request) -&gt; HttpResponse</code></h3>

<p><strong>What it does</strong>: Handles question queries and returns answers.</p>

<p><strong>Parameters</strong>:</p>
<ul>
<li><code>request</code>: Django HttpRequest with question</li>
</ul>

<p><strong>Process</strong>:</p>
<p>1. Check if POST request</p>
<p>2. Validate form with <code>AskForm</code></p>
<p>3. If valid:</p>
<ul>
<li>Get raw config</li>
<li>Initialize RAG system</li>
<li>Load knowledge base from disk</li>
<li>Call <code>rag.answer_question(question)</code></li>
<li>Return answer with context and sources</li>
</ul>
<p>4. If invalid or error: Return error message</p>

<p><strong>Key points</strong>:</p>
<ul>
<li>Loads existing KB (doesn't rebuild)</li>
<li>Fast: 2-5 seconds (loading) + search + QA</li>
<li>Returns answer with confidence and sources</li>
</ul>

<p><strong>Returns</strong>: HTML template with question, answer, confidence, and sources</p>

<p><strong>HTTP method</strong>: POST</p>

<p><strong>Example</strong>: User asks &quot;What is the payment deadline?&quot; → Returns answer from contracts</p>

<hr>

<h2>&lt;a name=&quot;query&quot;&gt;&lt;/a&gt;5. query.py</h2>

<p><strong>Purpose</strong>: Command-line interface for interactive querying of the knowledge base.</p>

<h3>Function: <code>main() -&gt; None</code></h3>

<p><strong>What it does</strong>: Runs interactive CLI loop for asking questions.</p>

<p><strong>Process</strong>:</p>
<p>1. Load YAML config</p>
<p>2. Wrap config in nested structure</p>
<p>3. Initialize RAG system</p>
<p>4. Load knowledge base from disk</p>
<p>5. Start interactive loop:</p>
<ul>
<li>Read user question</li>
<li>Check for 'exit'/'quit'</li>
<li>Call <code>rag.answer_question()</code></li>
<li>Display answer, confidence, sources</li>
<li>Loop back</li>
</ul>

<p><strong>Main loop</strong>:</p>
<pre><code class="language-text">User input → question
    → rag.answer_question()
    → Display results → Loop</code></pre>

<p><strong>Features</strong>:</p>
<ul>
<li>Type 'exit' or 'quit' to stop</li>
<li>Shows answer + confidence + sources</li>
<li>Continues until user exits</li>
</ul>

<p><strong>Returns</strong>: None</p>

<p><strong>How to run</strong>:</p>
<pre><code class="language-bash">python query.py</code></pre>

<p><strong>Example interaction</strong>:</p>
<pre><code class="language-text">Question: What are the payment terms?
Searching...

Answer: Payment is due within 30 days of invoice date
Confidence: 87.00%
Sources: contract1.txt, contract2.txt</code></pre>

<hr>

<h2>&lt;a name=&quot;setup_knowledge_base&quot;&gt;&lt;/a&gt;6. setup_knowledge_base.py</h2>

<p><strong>Purpose</strong>: Command-line script to build knowledge base from raw contract files.</p>

<h3>Function: <code>main() -&gt; None</code></h3>

<p><strong>What it does</strong>: One-time setup to build entire knowledge base.</p>

<p><strong>Process</strong>:</p>
<p>1. Load YAML config</p>
<p>2. Wrap config in nested structure</p>
<p>3. Initialize RAG system</p>
<p>4. Get data path from config (default <code>data/raw</code>)</p>
<p>5. Call <code>rag.build_knowledge_base(data_path)</code></p>
<p>6. Print success message</p>

<p><strong>Duration</strong>: 30-60 seconds depending on:</p>
<ul>
<li>Number of files</li>
<li>Total text size</li>
<li>Model download (first run only)</li>
</ul>

<p><strong>Creates</strong>:</p>
<ul>
<li><code>models/faiss_index.bin</code>: Embedding vectors in FAISS format</li>
<li><code>models/metadata.pkl</code>: Chunk metadata and text</li>
</ul>

<p><strong>Returns</strong>: None</p>

<p><strong>How to run</strong>:</p>
<pre><code class="language-bash">python setup_knowledge_base.py</code></pre>

<p><strong>Example output</strong>:</p>
<pre><code class="language-text">============================================================
BUILDING KNOWLEDGE BASE
============================================================

Loading embedding model: sentence-transformers/all-MiniLM-L6-v2
✓ Model loaded. Embedding dimension: 384
Found 5 contract files
✓ Loaded: contract1.txt
✓ Loaded: contract2.txt
...
Created 47 chunks from 5 documents
Encoding 47 chunks...
Building FAISS index with 47 embeddings...
✓ Index built successfully

============================================================
✓ Knowledge base built successfully!
============================================================</code></pre>

<hr>

<h2>Quick Reference: Function Call Flow</h2>

<h3>Building KB:</h3>
<pre><code class="language-text">setup_knowledge_base.py (main)
  → ContractRAG.build_knowledge_base()
    → DataProcessor.process_pipeline()
      → load_documents()
      → chunk_documents()
    → EmbeddingManager.encode_documents()
    → EmbeddingManager.build_index()
    → EmbeddingManager.save_index()</code></pre>

<h3>Querying (Web):</h3>
<pre><code class="language-text">Django: upload view
  → ContractRAG.build_knowledge_base() [same as above]

Django: ask view
  → ContractRAG.load_knowledge_base()
    → EmbeddingManager.load_index()
  → ContractRAG.answer_question()
    → retrieve_context()
      → EmbeddingManager.search()
    → QA pipeline extraction</code></pre>

<h3>Querying (CLI):</h3>
<pre><code class="language-text">query.py (main)
  → ContractRAG.__init__()
  → ContractRAG.load_knowledge_base()
  → Loop: ContractRAG.answer_question()</code></pre>

<hr>

<h2>Configuration Structure</h2>

<p>All functions expect this config structure (from <code>config/config.yaml</code> + wrapping):</p>

<pre><code class="language-python">{
    'data': {
        'chunk_size': 400,          # Characters per chunk
        'chunk_overlap': 50         # Overlap between chunks
    },
    'embedding': {
        'model_name': 'sentence-transformers/all-MiniLM-L6-v2',
        'faiss_index_path': 'models/faiss_index.bin',
        'metadata_path': 'models/metadata.pkl'
    },
    'rag': {
        'top_k': 3,                 # Default retrieve top-k
        'similarity_threshold': 0.0 # Min similarity score
    },
    'model': {
        'device': 'cpu'             # 'cpu' or 'cuda'
    }
}</code></pre>

<hr>

<h2>Summary Table</h2>

<p>| File | Function/Class | Purpose | Key Methods |</p>
<p>|------|--------|---------|-------------|</p>
<p>| <code>data_processor.py</code> | DataProcessor | Load/chunk documents | <code>load_documents()</code>, <code>clean_text()</code>, <code>chunk_documents()</code>, <code>process_pipeline()</code> |</p>
<p>| <code>embeddings.py</code> | EmbeddingManager | Embeddings &amp; search | <code>encode_documents()</code>, <code>build_index()</code>, <code>search()</code>, <code>save_index()</code>, <code>load_index()</code> |</p>
<p>| <code>rag_system.py</code> | ContractRAG | RAG orchestration | <code>build_knowledge_base()</code>, <code>load_knowledge_base()</code>, <code>retrieve_context()</code>, <code>answer_question()</code> |</p>
<p>| <code>views.py</code> | Django views | Web interface | <code>index()</code>, <code>upload()</code>, <code>ask()</code>, <code>get_rag_config()</code> |</p>
<p>| <code>query.py</code> | CLI | Interactive queries | <code>main()</code> |</p>
<p>| <code>setup_knowledge_base.py</code> | Setup | Build KB | <code>main()</code> |</p>


    </div>
</body>
</html>
